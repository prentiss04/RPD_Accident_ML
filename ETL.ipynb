{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'C:/Users/ruchi/Desktop/Berkley Extension Learning Docs/Final Project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Kaggle Data\n",
    "kaggle_metadata = pd.read_csv(f'{file_dir}/US_Accidents_Dec19.tar.gz', compression='gzip', error_bad_lines=False, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn off SettingWithCopyWarning ##############\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total rows extraced\n",
    "len(kaggle_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only Columns relevant for analysis\n",
    "df_subset = kaggle_metadata[['US_Accidents_Dec19.csv','Severity','Start_Time','End_Time',\n",
    "                             'Start_Lat','Start_Lng','Distance(mi)', 'Street','Side','City',\n",
    "                             'County','State','Zipcode','Timezone', \n",
    "                             'Temperature(F)','Humidity(%)','Pressure(in)',\n",
    "                             'Visibility(mi)','Wind_Direction','Wind_Speed(mph)','Precipitation(in)',\n",
    "                             'Weather_Condition','Amenity','Crossing','Junction','Railway',\n",
    "                             'Station','Stop','Traffic_Signal','Civil_Twilight'\n",
    "                            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Null Values\n",
    "df_subset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA with zero values for Precipitation column\n",
    "df_subset[\"Precipitation(in)\"].fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with other NA values\n",
    "df_subset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting dataset length\n",
    "len(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe on Severity so that when removing duplicates the one with higher severity is retained\n",
    "sorted_df = df_subset.sort_values('Severity',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many duplicates exist in the dataset\n",
    "len(sorted_df[['Severity', 'Start_Time', 'Start_Lat', 'Start_Lng']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "sorted_df.drop_duplicates(subset=['Severity', 'Start_Time', 'Start_Lat', 'Start_Lng'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first 5 digits of zipcode where zip code is in postal format of ZIP-4\n",
    "sorted_df['Zipcode'] = sorted_df['Zipcode'].str.replace(r\"-.*\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of the remaining dataset after removing duplicate and Null value rows\n",
    "len(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes for corrections\n",
    "sorted_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['Start_Time'] = pd.to_datetime(sorted_df.Start_Time)\n",
    "sorted_df['End_Time'] = pd.to_datetime(sorted_df.End_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Datatype for Severity from float to integer\n",
    "sorted_df['Severity'] = sorted_df['Severity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################   Create Highway Column   ####################################\n",
    "\n",
    "\n",
    "searchfor = ['highway', 'Tollway', 'expy', 'fwy', 'hwy', 'Interstate', \n",
    "             'Tpke', 'Pkwy', 'Parkway', '-', 'US', 'Route', \n",
    "             'FM', 'Byp', 'Trwy', 'Beltway', 'Skyway', 'Skwy', ]\n",
    "sorted_df.loc[sorted_df['Street'].str.contains('|'.join(searchfor), case=False), 'Highway'] = 'Y'\n",
    "\n",
    "# Fill NA with zero values for Precipitation column\n",
    "sorted_df[\"Highway\"].fillna('N', inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sorted_df[['Start_Lat','Start_Lng']][sorted_df['Highway'] == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Coordinates column\n",
    "sorted_df['Coordinates'] = sorted_df['Start_Lat'].map(str) + ':' + sorted_df['Start_Lng'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "sorted_df = sorted_df.rename(index=str,columns={'US_Accidents_Dec19.csv': 'Accident_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes for Loading into SQL Tables \n",
    "\n",
    "table1_df = sorted_df[['Accident_ID','Severity','Start_Time','End_Time',\n",
    "                             'Start_Lat','Start_Lng','Coordinates', 'Distance(mi)', 'Side', \n",
    "                             'Temperature(F)','Humidity(%)','Pressure(in)',\n",
    "                             'Visibility(mi)','Wind_Direction','Wind_Speed(mph)','Precipitation(in)',\n",
    "                             'Weather_Condition','Amenity','Crossing','Junction','Railway',\n",
    "                             'Station','Stop','Traffic_Signal','Civil_Twilight'\n",
    "                             ]]\n",
    "table2_df = sorted_df[['Coordinates', 'Street','City','County','State','Zipcode',\n",
    "                       'Timezone', 'Highway']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_df.drop_duplicates(subset=['Coordinates'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataframe columns to match the table column names\n",
    "table2_df.columns = map(str.lower, table2_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataframe columns to match the table column names\n",
    "table1_df = table1_df.rename(index=str,columns={'Distance(mi)':'distance', \n",
    "                             'Temperature(F)':'temperature',\n",
    "                             'Humidity(%)':'humidity',\n",
    "                             'Pressure(in)':'pressure',\n",
    "                             'Visibility(mi)':'visibility',\n",
    "                             'Wind_Speed(mph)':'wind_speed',\n",
    "                             'Precipitation(in)':'precipitation',\n",
    "                             })\n",
    "\n",
    "table1_df.columns = map(str.lower, table1_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Index\n",
    "table1_df.set_index('accident_id', inplace=True)\n",
    "table2_df.set_index('coordinates', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since very large dataset Create CSVs to load data into Postgres SQL tables\n",
    "table1_df.to_csv('table1.csv', sep='|')\n",
    "table2_df.to_csv('table2.csv',sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQL Load dependencies\n",
    "#from sqlalchemy import create_engine\n",
    "#import sqlalchemy as db\n",
    "import psycopg2\n",
    "from config import db_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"accident-viz.c4cdhyeva5ut.us-east-1.rds.amazonaws.com\", \n",
    "    port='5432', \n",
    "    dbname=\"Accident-ETL\", \n",
    "    user=\"postgres\", \n",
    "    password=db_password\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table2.csv', 'r') as f:\n",
    "    next(f) # Skip the header row.\n",
    "    cur.copy_from(f, 'accident_location', sep='|')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table1.csv', 'r') as f:\n",
    "    next(f) # Skip the header row.\n",
    "    cur.copy_from(f, 'accidents', sep='|')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Date and Time Column into Date, Time, Time in Seconds, and Day of week columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value Start date time column\n",
    "newstart = sorted_df[\"Start_Time\"].str.split(\" \",expand = True) \n",
    "  \n",
    "# making separate Start Time column from new data frame \n",
    "sorted_df[\"Start_Time_of_Day\"]= newstart[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value End date time column\n",
    "newend = sorted_df[\"End_Time\"].str.split(\" \",expand = True) \n",
    "\n",
    "# making separate Start Time column from new data frame \n",
    "sorted_df[\"End_Time_of_Day\"]= newend[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time to seconds for Start Time and End Time\n",
    "sorted_df['Start_seconds'] = pd.to_timedelta(sorted_df['Start_Time_of_Day']).dt.seconds\n",
    "\n",
    "# Convert Time to seconds for Start Time and End Time\n",
    "sorted_df['End_seconds'] = pd.to_timedelta(sorted_df['End_Time_of_Day']).dt.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['Start_Time'] = pd.to_datetime(sorted_df.Start_Time)\n",
    "sorted_df['End_Time'] = pd.to_datetime(sorted_df.End_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Day of the week for the accident\n",
    "sorted_df['Day_of_Week'] = sorted_df['Start_Time'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df[sorted_df['US_Accidents_Dec19.csv'] == 'A-2782717']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head(100).to_csv('Sample_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
