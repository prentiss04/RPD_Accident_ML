{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23895,
     "status": "ok",
     "timestamp": 1583446529083,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "q13u1Qd-xPGF",
    "outputId": "2be7bb66-af19-45ea-995a-7783dfb35e39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-2.4.5-bin-hadoop2.7.tgz'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7003d28d9af6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
    "\n",
    "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BigDataHW\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V13VEFGpxYx3"
   },
   "outputs": [],
   "source": [
    "# Configure settings for RDS\n",
    "#mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://accident-viz.c4cdhyeva5ut.us-east-1.rds.amazonaws.com:5432/Accident-ETL\"\n",
    "config = {\"user\":\"postgres\",\n",
    "          \"password\": \"traffic2020\",\n",
    "          \"driver\":\"org.postgresql.Driver\"}\n",
    "\n",
    "#spark.sparkContext.addFile(jdbc_url)\n",
    "#beauty_data_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Beauty_v1_00.tsv.gz\"), sep=\"\\t\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdrKlWT3xfOV"
   },
   "outputs": [],
   "source": [
    "# Get the id, age where age = 22 in SQL\n",
    "\n",
    "#sqlquery = \"select name,salary from testdb.employee\"\n",
    "#df_select = spark.read.jdbc(url=jdbc_url,table=_sqlquery,properties=db_properties=config)\n",
    "\n",
    "#sqlquery = \"(SELECT * FROM accidents where temperature>'90.0' as accdnts)\"\n",
    "sql_query = \"(SELECT * FROM accidents) as accdnts\"\n",
    "\n",
    "spark_df = spark.read.jdbc(url=jdbc_url, table = sql_query, column = 'severity', lowerBound =  1, upperBound = 4, numPartitions = 4, properties=config)\n",
    "\n",
    "#spark_df = spark.read.jdbc(url=jdbc_url, table = sqlquery, properties=config)\n",
    "\n",
    "#spark_df = spark.sql(\"SELECT * FROM accidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uQaFlwwzE0G"
   },
   "outputs": [],
   "source": [
    "######## Convert Spark DF to Pandas DF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7Ik6Yc1Pbcj"
   },
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a pandas DataFrame using Arrow\n",
    "pandas_df = pd.DataFrame()\n",
    "#result_pdf = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krISMqy4U6zX"
   },
   "outputs": [],
   "source": [
    "n = spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-M-_HL-KQP7d"
   },
   "outputs": [],
   "source": [
    "n = spark_df.count()\n",
    "chunksize = 80000\n",
    "for i in range (0, n, chunksize):\n",
    "  chunk = spark_df.filter(spark_df[\"rowid\"].between(i, i + chunksize))\n",
    "  pd_df = chunk.toPandas()\n",
    "  pandas_df= pandas_df.append(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1583446985721,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "abTqjmH_C0YP",
    "outputId": "7fd9030b-9965-4d24-8890-7bdf9e365418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494249"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of the converted DataFrame\n",
    "len(pandas_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6346,
     "status": "ok",
     "timestamp": 1583447538079,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "smVu_RV2RRsw",
    "outputId": "8b74aa23-34a9-4835-f5c5-e02a602c3e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+-------------------+------------------+-------------------+--------------------+--------+----+-----------+--------+--------+----------+--------------+----------+-------------+-----------------+-------+--------+--------+-------+-------+-----+--------------+--------------+-------+\n",
      "|accident_id|severity|         start_time|           end_time|         start_lat|          start_lng|         coordinates|distance|side|temperature|humidity|pressure|visibility|wind_direction|wind_speed|precipitation|weather_condition|amenity|crossing|junction|railway|station| stop|traffic_signal|civil_twilight|  rowid|\n",
      "+-----------+--------+-------------------+-------------------+------------------+-------------------+--------------------+--------+----+-----------+--------+--------+----------+--------------+----------+-------------+-----------------+-------+--------+--------+-------+-------+-----+--------------+--------------+-------+\n",
      "|   A-332457|       1|2017-01-30 18:03:16|2017-01-30 18:33:16|         32.729889|         -97.062614|32.729889:-97.062614|    0.01|   R|       67.5|    20.0|   30.06|      10.0|           SSW|       4.6|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096044|\n",
      "|  A-1197944|       1|2019-01-28 15:06:16|2019-01-28 16:36:16|43.034591999999996| -89.41085799999999|43.03459199999999...|     0.0|   R|       12.0|    84.0|   29.69|       1.0|           NNW|      13.8|          0.0|       Light Snow|  False|   False|   False|  False|  False|False|         False|           Day|2096045|\n",
      "|   A-592383|       1|2019-10-16 07:57:06|2019-10-16 08:57:21|29.837509000000004|         -95.640556|29.83750900000000...|     0.0|   L|       75.0|   100.0|   29.79|       8.0|             N|      12.0|          0.0|           Cloudy|  False|   False|   False|  False|  False|False|         False|           Day|2096046|\n",
      "|  A-1822452|       1|2018-02-04 10:40:30|2018-02-04 11:10:00|42.358608000000004|         -83.076408|42.35860800000000...|     0.0|   R|       33.1|    92.0|   29.79|       2.0|            SW|       8.1|         0.01|       Light Snow|  False|   False|   False|  False|  False|False|         False|           Day|2096047|\n",
      "|  A-1316014|       1|2018-11-27 11:32:20|2018-11-27 12:16:48|30.339596000000004| -81.77271999999999|30.33959600000000...|     0.0|   L|       55.4|    41.0|   29.99|      10.0|          West|      10.4|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096048|\n",
      "|   A-240094|       1|2016-11-04 13:24:03|2016-11-04 13:54:03|          38.90448|         -77.476463| 38.90448:-77.476463|    0.01|   R|       62.1|    39.0|   30.22|      10.0|           NNW|      12.7|          0.0| Scattered Clouds|  False|   False|   False|  False|  False|False|         False|           Day|2096049|\n",
      "|      A-961|       1|2016-06-22 23:54:48|2016-06-23 00:39:48|         37.750488|-121.37998200000001|37.750488:-121.37...|     0.0|   R|       66.0|    42.0|   29.94|      10.0|           WNW|       6.9|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|         Night|2096050|\n",
      "|   A-122164|       1|2017-01-12 06:50:31|2017-01-12 07:20:31|          28.02314|         -82.244713| 28.02314:-82.244713|    0.01|   R|       53.6|   100.0|   30.33|       0.5|          East|       3.5|          0.0|              Fog|  False|   False|   False|  False|  False|False|         False|         Night|2096051|\n",
      "|  A-1827600|       1|2018-02-06 06:45:37|2018-02-06 07:15:37|         32.707951|         -96.431053|32.707951:-96.431053|     0.0|   L|       37.4|    93.0|   30.17|       5.0|         North|       8.1|          0.0|         Overcast|  False|   False|   False|  False|  False|False|         False|         Night|2096052|\n",
      "|  A-1627128|       1|2018-04-30 18:17:21|2018-04-30 18:47:21|33.258559999999996|         -86.851845|33.25855999999999...|     0.0|   L|       75.9|    32.0|   30.17|      10.0|           ESE|       4.6|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096053|\n",
      "|  A-1476748|       1|2018-08-13 21:02:15|2018-08-13 21:31:58|         30.198021|         -97.750099|30.198021:-97.750099|     0.0|   R|       87.1|    46.0|   29.89|      10.0|           SSE|      13.8|          0.0| Scattered Clouds|  False|   False|   False|  False|  False|False|         False|         Night|2096054|\n",
      "|  A-1594540|       1|2018-06-14 17:56:16|2018-06-14 18:26:16|         41.450733| -81.70118000000001|41.450733:-81.701...|     0.0|   L|       75.0|    55.0|   30.01|      10.0|          West|      10.4|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096055|\n",
      "|   A-842966|       1|2019-08-07 08:17:54|2019-08-07 09:02:33|          27.82093|         -82.753922| 27.82093:-82.753922|     0.0|   L|       80.0|   100.0|   29.99|      10.0|          CALM|       0.0|          0.0|             Fair|  False|   False|   False|  False|  False|False|          True|           Day|2096056|\n",
      "|  A-1627124|       1|2018-04-30 17:46:44|2018-04-30 18:16:44|          33.25642|         -86.855942| 33.25642:-86.855942|     0.0|   L|       75.9|    32.0|   30.17|      10.0|           ESE|       4.6|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096057|\n",
      "|   A-472245|       1|2017-06-27 11:34:45|2017-06-27 12:04:45|         41.587936|         -87.525436|41.587936:-87.525436|     0.0|   R|       70.5|    38.0|   30.14|      10.0|          West|       9.2|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096058|\n",
      "|  A-1196311|       1|2019-01-27 18:36:28|2019-01-27 19:06:28|         32.678406| -97.10619399999999|32.678406:-97.106...|     0.0|   L|       54.0|    35.0|    30.0|      10.0|           SSW|       3.5|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|         Night|2096059|\n",
      "|  A-1641340|       1|2018-05-07 16:47:47|2018-05-07 17:32:47|         42.200428|         -87.915291|42.200428:-87.915291|     0.0|   L|       71.1|    32.0|   30.16|      10.0|           ESE|      10.4|          0.0|            Clear|  False|   False|   False|  False|  False|False|         False|           Day|2096060|\n",
      "|   A-379617|       1|2017-02-27 08:20:59|2017-02-27 08:50:59|          38.65517|         -76.999123| 38.65517:-76.999123|    0.01|   R|       39.0|    85.0|   30.44|      10.0|            SW|       8.1|          0.0|         Overcast|  False|   False|   False|  False|  False|False|          True|           Day|2096061|\n",
      "|   A-262986|       1|2016-12-08 13:46:18|2016-12-08 15:00:26|          29.65737|         -95.555557| 29.65737:-95.555557|    0.01|   R|       48.0|    74.0|   30.39|      10.0|         North|      17.3|          0.0|         Overcast|  False|   False|   False|  False|  False|False|         False|           Day|2096062|\n",
      "|   A-680391|       1|2019-11-26 06:51:04|2019-11-26 07:50:36|         37.618195|        -122.152748|37.618195:-122.15...|     0.0|   R|       40.0|    70.0|   30.01|      10.0|           ENE|       5.0|          0.0|    Partly Cloudy|  False|   False|   False|  False|  False|False|         False|           Day|2096063|\n",
      "+-----------+--------+-------------------+-------------------+------------------+-------------------+--------------------+--------+----+-----------+--------+--------+----------+--------------+----------+-------------+-----------------+-------+--------+--------+-------+-------+-----+--------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10059,
     "status": "ok",
     "timestamp": 1583447587931,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "OSYbZW_6RyNM",
    "outputId": "4b7dac46-82ed-4f06-d059-268e6ae0859b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 2494218\n"
     ]
    }
   ],
   "source": [
    "# Determine the size of the dataframe\n",
    "print(len(spark_df.columns), spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_GSJVbXQC1d"
   },
   "outputs": [],
   "source": [
    "# read in accident_location table\n",
    "sqlquery_location = \"(SELECT * FROM accident_location where state='CA') as locations\"\n",
    "spark_loc_df = spark.read.jdbc(url=jdbc_url, table = sqlquery_location, properties=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2119,
     "status": "ok",
     "timestamp": 1583447667483,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "RahKr36oRjK9",
    "outputId": "3f5a8abf-b1ac-474f-d50b-236f6da3b71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+------------+-----+-------+----------+-------+\n",
      "|         coordinates|             street|           city|      county|state|zipcode|  timezone|highway|\n",
      "+--------------------+-------------------+---------------+------------+-----+-------+----------+-------+\n",
      "|36.73457:-120.199785| W Whitesbridge Ave|         Kerman|      Fresno|   CA|  93630|US/Pacific|      N|\n",
      "| 37.31641:-121.95618|            I-280 S|       San Jose| Santa Clara|   CA|  95117|US/Pacific|      Y|\n",
      "|36.73455:-120.205...| W Whitesbridge Ave|   Tranquillity|      Fresno|   CA|  93668|US/Pacific|      N|\n",
      "| 37.31469:-121.94078|            CA-17 S|       San Jose| Santa Clara|   CA|  95128|US/Pacific|      Y|\n",
      "|37.31485999999999...|     Sinclair Fwy N|       San Jose| Santa Clara|   CA|  95126|US/Pacific|      Y|\n",
      "|37.3172:-121.9320...|       S Bascom Ave|       San Jose| Santa Clara|   CA|  95128|US/Pacific|      N|\n",
      "|37.87589000000000...|      Camino Diablo|      Brentwood|Contra Costa|   CA|  94513|US/Pacific|      N|\n",
      "| 38.44526:-122.60505|         Sonoma Hwy|     Santa Rosa|      Sonoma|   CA|  95409|US/Pacific|      Y|\n",
      "|38.443937:-122.60...|         Sonoma Hwy|     Santa Rosa|      Sonoma|   CA|  95409|US/Pacific|      Y|\n",
      "| 39.42589:-121.68817|   State Highway 99|          Biggs|       Butte|   CA|  95917|US/Pacific|      Y|\n",
      "|39.407671:-121.68802|   State Highway 99|          Biggs|       Butte|   CA|  95917|US/Pacific|      Y|\n",
      "|37.31589:-121.915...|            I-280 N|       San Jose| Santa Clara|   CA|  95128|US/Pacific|      Y|\n",
      "|37.31535:-121.914...|            I-280 N|       San Jose| Santa Clara|   CA|  95126|US/Pacific|      Y|\n",
      "|34.15005999999999...|            I-405 N|   Sherman Oaks| Los Angeles|   CA|  91403|US/Pacific|      Y|\n",
      "| 37.31666:-121.94476|            I-280 S|       San Jose| Santa Clara|   CA|  95128|US/Pacific|      Y|\n",
      "| 33.92185:-118.20341|            I-105 E|        Lynwood| Los Angeles|   CA|  90262|US/Pacific|      Y|\n",
      "| 37.31769:-121.93535|     Sinclair Fwy N|       San Jose| Santa Clara|   CA|  95128|US/Pacific|      Y|\n",
      "|37.29134000000000...|            CA-17 N|       Campbell| Santa Clara|   CA|  95008|US/Pacific|      Y|\n",
      "|34.12622800000000...|     Foothill Fwy W|          Azusa| Los Angeles|   CA|  91702|US/Pacific|      Y|\n",
      "|38.853086:-121.78...|   State Highway 45|Knights Landing|        Yolo|   CA|  95645|US/Pacific|      Y|\n",
      "+--------------------+-------------------+---------------+------------+-----+-------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_loc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1583447677467,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "yvNXQ3fER7N-",
    "outputId": "a092274c-1a18-4c74-b958-80b4cbdfcdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 201190\n"
     ]
    }
   ],
   "source": [
    "# Determine the size of the dataframe\n",
    "print(len(spark_loc_df.columns), spark_loc_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQoxv6mvRIvV"
   },
   "source": [
    "Create first frame to read in all accidents with severity = 4\n",
    "Create second frame to read in all accident locations = CA\n",
    "Create union DF between the two frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 255536,
     "status": "error",
     "timestamp": 1583447945082,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "gDRqOWTeR7dz",
    "outputId": "267dbc23-6713-4bfd-a283-f6fae8f390a2"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7f13752f9d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccident_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_loc_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#    , lsuffix='*acc', rsuffix=\"*acc_loc\", sort='False')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccident_join\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o533.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 45.0 failed 1 times, most recent failure: Lost task 2.0 in stage 45.0 (TID 119, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 192397 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "#DataFrame.join(self, other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "accident_join = spark_df.join(spark_loc_df, on='coordinates', how='inner')#    , lsuffix='*acc', rsuffix=\"*acc_loc\", sort='False')\n",
    "accident_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25dXP0cR7lB"
   },
   "outputs": [],
   "source": [
    "# Determine the size of the dataframe\n",
    "print(len(accident_join.columns), accident_join.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I6JQWfTzelX"
   },
   "source": [
    "# Determine which columns to drop, add and which to create dummies\n",
    "### separate start_time/end_time into components (date & time)\n",
    "### net end_time from start_time (ensure that start_date = end_date in case accident occurs near midnight)\n",
    "### create dummies for side (R & L) or reassign to 0, 1\n",
    "### create dummies for wind_direction\n",
    "### create dummies for weather_condition\n",
    "### create 0,1 for amenity, crossing, junction, railway, station, stop, traffic_signal and possibly civil_twilight\n",
    "### create dummies for timezone\n",
    "### create 0,1 for highway\n",
    "### Scale inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hnL-AzuR7KU"
   },
   "outputs": [],
   "source": [
    "accident_join.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trVroDJvR7U7"
   },
   "outputs": [],
   "source": [
    "# Copy accident_join\n",
    "#X_df = accident_join.select('*').toPandas()\n",
    "#X_df = accident_join.toPandas()\n",
    "X_df = accident_join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7kQroX57Ksp"
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "\n",
    "columns_to_drop = ['coordinates', 'accident_ID', 'street', 'city', 'county']\n",
    "X_df = spark_df.drop(*columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1583448354344,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "zj-yILgz1TzQ",
    "outputId": "8cb831a2-b309-4687-881d-77ef72eeaf41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('severity', 'int'),\n",
       " ('start_time', 'timestamp'),\n",
       " ('end_time', 'timestamp'),\n",
       " ('start_lat', 'double'),\n",
       " ('start_lng', 'double'),\n",
       " ('distance', 'float'),\n",
       " ('side', 'string'),\n",
       " ('temperature', 'float'),\n",
       " ('humidity', 'float'),\n",
       " ('pressure', 'float'),\n",
       " ('visibility', 'float'),\n",
       " ('wind_direction', 'string'),\n",
       " ('wind_speed', 'string'),\n",
       " ('precipitation', 'string'),\n",
       " ('weather_condition', 'string'),\n",
       " ('amenity', 'string'),\n",
       " ('crossing', 'string'),\n",
       " ('junction', 'string'),\n",
       " ('railway', 'string'),\n",
       " ('station', 'string'),\n",
       " ('stop', 'string'),\n",
       " ('traffic_signal', 'string'),\n",
       " ('civil_twilight', 'string'),\n",
       " ('rowid', 'int')]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.dtypes\n",
    "\n",
    "#X_df.filter(X_df(\"distance\")>0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nw7IdGX06nk-"
   },
   "outputs": [],
   "source": [
    "X_dummies_df = pd.get_dummies(X_df, columns=[\"amenity\"])\n",
    "\n",
    "#X_dummies_df = pd.get_dummies(X_df, columns=[\"side\", \"wind_direction\", \"weather_condition\", \"amenity\", \"crossing\", \"junction\", \"railway\", \"station\", \"stop\", \"traffic_signal\", \"civil_twilight\", \"timezone\", \"highway\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1583449060598,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "muR2oWud6tpG",
    "outputId": "69ad1185-d55c-432e-f927-939e9fff2e14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataFrame[severity: int, start_time: timestamp, end_time: timestamp, start_lat: double, start_lng: double, distance: float, side: string, temperature: float, humidity: float, pressure: float, visibility: float, wind_direction: string, wind_speed: string, precipitation: string, weather_condition: string, amenity: string, crossing: string, junction: string, railway: string, station: string, stop: string, traffic_signal: string, civil_twilight: string, rowid: int]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DataFrame[severity: int, start_time: timestamp, end_time: timestamp, start_lat: double, start_lng: double, distance: float, side: string, temperature: float, humidity: float, pressure: float, visibility: float, wind_direction: string, wind_speed: string, precipitation: string, weather_condition: string, amenity: string, crossing: string, junction: string, railway: string, station: string, stop: string, traffic_signal: string, civil_twilight: string, rowid: int]\n",
       "0                                                  1                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 409237,
     "status": "error",
     "timestamp": 1583448813166,
     "user": {
      "displayName": "Prentiss Douglass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhi66I1t1hQtm28lpUFxkmxG7ZXDCAR720ig4csA=s64",
      "userId": "17218080498531010411"
     },
     "user_tz": 480
    },
    "id": "ktIVC1igAPp5",
    "outputId": "0c5f6ad1-a77e-4c8a-d4b5-cdb51d9c95aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py:2146: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o595.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n",
      "\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\n",
      "\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\n",
      "\tat sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 47.0 failed 1 times, most recent failure: Lost task 1.0 in stage 47.0 (TID 121, localhost, executor driver): java.lang.NegativeArraySizeException\n",
      "\tat org.postgresql.core.PGStream.receiveTupleV3(PGStream.java:449)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2204)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:447)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:368)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:158)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3318)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\n",
      "\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\n",
      "\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\n",
      "\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\n",
      "Caused by: java.lang.NegativeArraySizeException\n",
      "\tat org.postgresql.core.PGStream.receiveTupleV3(PGStream.java:449)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2204)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:447)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:368)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:158)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-06487fc6c007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_dummies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_dummies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2127\u001b[0m                         \u001b[0m_check_dataframe_localize_timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m                     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collectAsArrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m                         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_collectAsArrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2189\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrowStreamSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2191\u001b[0;31m                 \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Join serving thread and raise any exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o595.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\n\tat sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 47.0 failed 1 times, most recent failure: Lost task 1.0 in stage 47.0 (TID 121, localhost, executor driver): java.lang.NegativeArraySizeException\n\tat org.postgresql.core.PGStream.receiveTupleV3(PGStream.java:449)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2204)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:447)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:368)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:158)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:108)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3318)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\n\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\nCaused by: java.lang.NegativeArraySizeException\n\tat org.postgresql.core.PGStream.receiveTupleV3(PGStream.java:449)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2204)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:447)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:368)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:158)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:108)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "X_dummies_df = (X_df).select(\"*\").toPandas()\n",
    "X_dummies_df = X_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ic-J1nwOKn52"
   },
   "outputs": [],
   "source": [
    "X_dummies_df = pd.get_dummies(X_dummies_df, columns=[\"amenity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMQXIDcMBEih"
   },
   "outputs": [],
   "source": [
    "X_dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BonzHNsOqycM"
   },
   "outputs": [],
   "source": [
    "#X_dummies_df.empty\n",
    "#X_dummies_df.columns\n",
    "X_df['side']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2-OapHK2O3O"
   },
   "outputs": [],
   "source": [
    "#X_df = pd.get_dummies(X_df, columns=[\"amenity\"])\n",
    "X_df.head()\n",
    "\n",
    "\n",
    "#X_df['amenity'] = X_df['amenity'].astype(object)#, columns=['amenity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ddeqqGR6QSP"
   },
   "outputs": [],
   "source": [
    "accident_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_76WixfD6sqa"
   },
   "outputs": [],
   "source": [
    "y = accident_join['severity']\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNauCG5w4G3c"
   },
   "outputs": [],
   "source": [
    "X_ps_df = SparkSession.createDataFrame(X_df).collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNp6fF34a/j1Pv6BWpiZ2bZ",
   "collapsed_sections": [],
   "name": "Accidents_py.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
