# -*- coding: utf-8 -*-
"""Accidents_py.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12dklUbCXdi9-ZE9FgMKKh1sh9uH_n0lZ
"""

# Install Java, Spark, and Findspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz

!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark
# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"
# Start a SparkSession
import findspark
findspark.init()
!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("BigDataHW").config("spark.driver.extraClassPath","/content/postgresql-42.2.9.jar").getOrCreate()

# Configure settings for RDS
#mode = "append"
jdbc_url="jdbc:postgresql://accident-viz.c4cdhyeva5ut.us-east-1.rds.amazonaws.com:5432/Accident-ETL"

# put following code into config file
#config = {"user":"XXXXX",
          #"password": "XXXXXXXX",
          #"driver":"XXXXXXXX"}

#spark.sparkContext.addFile(jdbc_url)
#beauty_data_df = spark.read.csv(SparkFiles.get("amazon_reviews_us_Beauty_v1_00.tsv.gz"), sep="\t", header=True)

# Get the id, age where age = 22 in SQL

#sqlquery = "select name,salary from testdb.employee"
#df_select = spark.read.jdbc(url=jdbc_url,table=_sqlquery,properties=db_properties=config)

#sqlquery = "(SELECT * FROM accidents where temperature>'90.0' as accdnts)"
sql_query = "(SELECT acc.*, acc_loc.highway, acc_loc.timezone FROM accidents acc JOIN accident_location acc_loc on acc.coordinates = acc_loc.coordinates) as accdnts"

spark_df = spark.read.jdbc(url=jdbc_url, table = sql_query, column = 'severity', lowerBound =  1, upperBound = 4, numPartitions = 4, properties=config)

#spark_df = spark.read.jdbc(url=jdbc_url, table = sqlquery, properties=config)

#spark_df = spark.sql("SELECT * FROM accidents")

######## Convert Spark DF to Pandas DF
import numpy as np
import pandas as pd
# Enable Arrow-based columnar data transfers
spark.conf.set("spark.sql.execution.arrow.enabled", "true")

# Convert the Spark DataFrame to a pandas DataFrame using Arrow
pandas_df = pd.DataFrame()
#result_pdf = spark_df.toPandas()

n = spark_df.count()

chunksize = 80000
for i in range (0, n, chunksize):
  chunk = spark_df.filter(spark_df["rowid"].between(i, i + chunksize))
  pd_df = chunk.toPandas()
  pandas_df= pandas_df.append(pd_df)

# Check the size of the converted DataFrame
len(pandas_df)

# Drop columns

#columns_to_drop = ['coordinates', 'accident_id']

X_df = pandas_df.drop(['coordinates','accident_id', 'rowid'], axis=1)

X_df.dtypes

#X_df.filter(X_df("distance")>0.5).show()

# Determine number of unique weather_conditions
X_df['weather_condition'].nunique()

weather_counts = X_df.weather_condition.value_counts()
weather_counts.head(50)

# Visualize the value plots
weather_counts.plot.density()

# for binary, lambda fxn true=1, f=0 (side, crossing, junction, amenity, railway, station, stop, traffic_signal, highway - check civil_twilight)
def convert_binary(feature):
  if "False" in feature:
    return 0
  else:
    return 1

X_df['side'] = X_df['side'].replace({'R':1, 'L':0})
X_df['crossing'] = X_df['crossing'].replace({'True':1, 'False':0})
X_df['highway'] = X_df['highway'].replace({'True':1, 'False':0})
X_df['junction'] = X_df['junction'].replace({'True':1, 'False':0})
X_df['amenity'] = X_df['amenity'].replace({'True':1, 'False':0})
X_df['railway'] = X_df['railway'].replace({'True':1, 'False':0})
X_df['station'] = X_df['station'].replace({'True':1, 'False':0})
X_df['stop'] = X_df['stop'].replace({'True':1, 'False':0})
X_df['traffic_signal'] = X_df['traffic_signal'].replace({'True':1, 'False':0})
X_df['civil_twilight'] = X_df['civil_twilight'].replace({'Night':1, 'Day':0})

X_df.head()

# Create separate columns for start/end date/time
X_df['new_start_date'] = [d.date() for d in X_df["start_time"]] 
X_df['new_start_time'] = [d.time() for d in X_df["start_time"]] 
  
X_df['new_end_date'] = [d.date() for d in X_df["end_time"]] 
X_df['new_end_time'] = [d.time() for d in X_df["end_time"]]

X_df.dtypes

X_df['new_start_date'] =  X_df['new_start_date'].astype('datetime64[ns]')
X_df['new_end_date'] =  X_df['new_end_date'].astype('datetime64[ns]')
#X_df[['new_start_date']] =  pd.to_datetime(X_df[['new_start_date']], format=)
#X_df[['new_start_date', 'new_end_time']] =  pd.to_datetime(X_df[['new_start_date', 'new_end_time']], format='%d%b%Y:%H:%M:%S.%f')

X_df.head()

# Create column for day of the week
X_df["start_day_of_week"] = X_df['new_start_date'].dt.weekday
X_df["end_day_of_week"] = X_df['new_end_date'].dt.weekday

X_df.head()

# Determine duration of traffic event
X_df['duration'] = X_df['end_time'] - X_df['start_time']
X_df.head()
# convert to seconds?

X_df['duration_seconds'] = X_df['duration'].dt.total_seconds()
X_df.head()

# Create DF of unique weather conditions and counts, write to csv for categorization 

from google.colab import files

weather_unique = X_df['weather_condition'].value_counts().rename_axis('unique_values').reset_index(name='counts')
weather_unique.to_csv('weather_condition.csv')

# Read in weather_condition_categories.csv file (just the weather_condition_categories tab not the pivot table tab)

# Create DF of unique weather conditions and counts, write to csv for categorization 

from google.colab import files
uploaded = files.upload()

import io
conditions_cat_df = pd.read_csv(io.BytesIO(uploaded['weather_condition_categories.csv']))

# create vlookup to match condition and provide weather category. Match on X_df['weather_condition'] = .csv['unique_values']

# drop weather_condition column

X_df = pd.merge(X_df, conditions_cat_df, how="left", left_on="weather_condition", right_on="unique_values")
X_df.head()

X_df.rename(columns= {'category': "weather_category"}, inplace=True)

#X_df['side'] = X_df['side'].astype(str).astype(float)
X_df['wind_speed'] = X_df['wind_speed'].astype(str).astype(float)
X_df['precipitation'] = X_df['precipitation'].astype(str).astype(float)
X_df['amenity'] = X_df['amenity'].astype(str).astype(float)
X_df['crossing'] = X_df['crossing'].astype(str).astype(float)
X_df['junction'] = X_df['junction'].astype(str).astype(float)
X_df['railway'] = X_df['railway'].astype(str).astype(float)
X_df['station'] = X_df['station'].astype(str).astype(float)
X_df['civil_twilight'] = X_df['civil_twilight'].astype(str).astype(float)
#X_df['highway'] = X_df['highway'].astype(str).astype(float)

X_df.dtypes

"""# Figure out what is going on with "side" and "highway". Can't convert to float"""

X_df.head()

X_df.drop(['start_time', 'end_time', 'weather_condition', 'new_start_date', 'new_start_time', 'new_end_date', 'new_end_time', 'Unnamed: 0','unique_values','counts', 'duration'], axis=1, inplace=True)
X_dr.head()

# apply get_dummies to wind_direction, timezone, weather_condition
#X_dummies_df = pd.get_dummies(X_df, columns=["wind_direction", "timezone", "weather_category", start_day_of_week"])
#X_dummies_df.head()

"""# Determine which columns to drop, add and which to create dummies

### Scale inputs
"""